{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBRFClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
        "from sklearn import metrics\n",
        "import spacy\n"
      ],
      "metadata": {
        "id": "v5l54xK5aCQ7"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "-9ZhV7efWYRi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/some datasets/Language Detection.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(100)"
      ],
      "metadata": {
        "id": "YDg8ZPnXaZem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Language\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxx85_xJZI99",
        "outputId": "a638a2d9-75b1-418d-de98-88d30f8f770b"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "English       1385\n",
              "French        1014\n",
              "Spanish        819\n",
              "Portugeese     739\n",
              "Italian        698\n",
              "Russian        692\n",
              "Sweedish       676\n",
              "Malayalam      594\n",
              "Dutch          546\n",
              "Arabic         536\n",
              "Turkish        474\n",
              "German         470\n",
              "Tamil          469\n",
              "Danish         428\n",
              "Kannada        369\n",
              "Greek          365\n",
              "Hindi           63\n",
              "Name: Language, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[\"Text\"]"
      ],
      "metadata": {
        "id": "VrISZ-YAdMoS"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y= df[\"Language\"]"
      ],
      "metadata": {
        "id": "n9IVQEC0nzzI"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def remove_regex_special_chars_digits(input_string):   # questa funzione è razzista, elimina lingue strane.\n",
        "#     return re.sub(r'[^a-zA-Z\\s]', '', input_string)\n",
        "\n",
        "# X=X[\"Text\"].apply(remove_regex_special_chars_digits)"
      ],
      "metadata": {
        "id": "9adY5h-YahAB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qui abbiamo creato una funzione che elimina i numeri e la applichiamo ad X ###\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Bu0O2gACELcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_regex_special_chars_digits(input_string):  \n",
        "  return re.sub(r'[0-9]', '', input_string)\n",
        "\n",
        "X=X.apply(remove_regex_special_chars_digits)"
      ],
      "metadata": {
        "id": "D4qwhi7xjshB"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRANSFORM VS FIT_TRANSFORM\n",
        "\n",
        "Qual è la differenza tra CountVectorizer().transform e CountVectorizer().fit_transform\n",
        "La differenza tra i metodi transform e fit_transform nella classe CountVectorizer riguarda il modo in cui il vettorizzatore (l'oggetto CountVectorizer) viene addestrato e utilizzato per trasformare un testo in un vettore numerico.\n",
        "\n",
        "Il metodo fit_transform effettua sia l'addestramento che la trasformazione del testo in input in una sola chiamata. Durante l'addestramento, il vettorizzatore apprende il vocabolario di tutte le parole presenti nel corpus di testo, ovvero la lista di tutte le parole che compariranno nella matrice di conteggi. Questo vocabolario viene utilizzato per trasformare il testo in una matrice di conteggi.\n",
        "\n",
        "Il metodo transform, invece, richiede che il vettorizzatore sia già stato addestrato tramite il metodo fit o fit_transform su un corpus di testo precedente. Questo metodo utilizza il vocabolario già appreso per trasformare il testo in input in una matrice di conteggi.\n",
        "\n",
        "In sintesi, se si vuole trasformare un testo in un vettore numerico utilizzando la tecnica \"Bag of Words\" senza addestrare il vettorizzatore, è necessario utilizzare il metodo fit o fit_transform prima di chiamare il metodo transform. Se, invece, si vuole addestrare il vettorizzatore e trasformare il testo in un solo passaggio, si può utilizzare il metodo fit_transform."
      ],
      "metadata": {
        "id": "4XZzK0g25Q-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## VECTORIZER CALCOLA LE BAG OF WORDS E X LO TRASFORIAMO IN UN VETTORE CODIFICATO\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X).toarray()"
      ],
      "metadata": {
        "id": "N1pedtrsdwZm"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CODIFICHIAMNO LA LABEL\n",
        "\n",
        "le = LabelEncoder()\n",
        "y =le.fit_transform(y)"
      ],
      "metadata": {
        "id": "_PqAeSreCheT"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the fit_transform method of the CountVectorizer object is used to learn the vocabulary from the training data and return the token count matrix. The vocabulary is stored in the vocabulary_ attribute of the CountVectorizer object.\n",
        "The resulting token count matrix X can then be used as input to a machine learning algorithm to perform text classification, information retrieval, or other NLP tasks."
      ],
      "metadata": {
        "id": "wFY9bkt2g4Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## DIVIDIAMO IN TRAIN E TEST\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "v2ExfT1mZUsW"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#APPLICHIAMO UN CLASSIFICATORE\n",
        "\n",
        "##MULTINOMIAL NB\n",
        "model=MultinomialNB()\n",
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZDj-GaooLaJ",
        "outputId": "43d80ce8-b253-4977-b27e-5b0fed78dcb8"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##PREVISIONI\n",
        "prediction=model.predict(x_test)"
      ],
      "metadata": {
        "id": "Z6XNBEfhprnM"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.confusion_matrix(y_test,prediction))\n",
        "print(metrics.classification_report(y_test,prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjxcK39ip5-A",
        "outputId": "d9cf96e6-76d3-4a8f-a605-a652f2847b99"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[104   0   0   2   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0  71   0   1   1   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0 108   2   0   0   0   0   0   0   0   0   0   1   0   0   0]\n",
            " [  0   0   0 291   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   1   1 216   0   0   0   0   0   0   0   0   1   0   0   0]\n",
            " [  0   1   0   2   0  90   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   1   0   0  67   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  10   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   1   0   0   0   0 144   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  66   0   0   0   0   0   0   0]\n",
            " [  0   0   0   2   0   0   0   0   0   0 119   0   0   0   0   0   0]\n",
            " [  0   0   0   3   0   0   0   0   0   0   0 141   0   0   0   0   0]\n",
            " [  0   0   0   1   0   0   0   0   0   0   0   0 135   0   0   0   0]\n",
            " [  0   0   0   3   0   0   0   0   0   0   0   1   0 156   0   0   0]\n",
            " [  0   1   0   1   0   0   0   0   0   0   0   0   0   0 131   0   0]\n",
            " [  0   0   0   1   0   0   0   0   0   0   0   0   0   0   0  86   0]\n",
            " [  0   0   1   4   2   0   0   0   0   0   0   0   0   0   0   0  98]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99       106\n",
            "           1       0.97      0.97      0.97        73\n",
            "           2       0.98      0.97      0.98       111\n",
            "           3       0.92      1.00      0.96       291\n",
            "           4       0.99      0.99      0.99       219\n",
            "           5       1.00      0.97      0.98        93\n",
            "           6       1.00      0.99      0.99        68\n",
            "           7       1.00      1.00      1.00        10\n",
            "           8       1.00      0.99      1.00       145\n",
            "           9       1.00      1.00      1.00        66\n",
            "          10       1.00      0.98      0.99       121\n",
            "          11       0.99      0.98      0.99       144\n",
            "          12       1.00      0.99      1.00       136\n",
            "          13       0.99      0.97      0.98       160\n",
            "          14       1.00      0.98      0.99       133\n",
            "          15       1.00      0.99      0.99        87\n",
            "          16       1.00      0.93      0.97       105\n",
            "\n",
            "    accuracy                           0.98      2068\n",
            "   macro avg       0.99      0.98      0.99      2068\n",
            "weighted avg       0.98      0.98      0.98      2068\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definisco una funzione che ritrasforma gli oggetti come in partenza ed effettua le previsioni dandogli in input una frase in qualsiasi lingua "
      ],
      "metadata": {
        "id": "L1XPq3aWFImb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def previsione(text):\n",
        "     x = vectorizer.transform([text]).toarray() # converting text to bag of words model (Vector)\n",
        "     lang = model.predict(x) # predicting the language\n",
        "     lang = le.inverse_transform(lang) # finding the language corresponding the the predicted value\n",
        "     print(\"The langauge is\",lang[0]) # printing the language"
      ],
      "metadata": {
        "id": "IuSoFERewfSe"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previsione(\"je m'appelle Noemi?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON8PlBQ36fke",
        "outputId": "bc8f1845-f443-4e5e-c046-485c00f7777a"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The langauge is French\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " |------|\n",
        " "
      ],
      "metadata": {
        "id": "iYCrugkSD32i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Da qui in poi ci sono altre prove fatte con altri modelli "
      ],
      "metadata": {
        "id": "WgoJpxovDrpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer1 = TfidfVectorizer()\n",
        "X1= vectorizer1.fit_transform(X).toarray()"
      ],
      "metadata": {
        "id": "kGxT7oXMtxxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1_train, x1_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "2LHv5rRaukfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERNOULLI NB\n",
        "model1=BernoulliNB()\n",
        "model1.fit(x_train, y_train)\n",
        "\n",
        "prediction1=model1.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPWmXycprHhI",
        "outputId": "27ff315b-7c13-425d-bdc4-a32ff0eb3ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.confusion_matrix(y_test,prediction1))\n",
        "print(metrics.classification_report(y_test,prediction1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFUaplKarN4T",
        "outputId": "39120dd9-bb07-45aa-f98d-a99e5714f832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 34   0   0  72   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0  15   0  56   0   0   0   0   0   0   0   0   0   0   2   0   0]\n",
            " [  0   0  31  80   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0 291   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0  22 197   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0  82   0  11   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0  61   0   0   7   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   7   0   0   0   0   0   1   0   0   1   0   0   1   0]\n",
            " [  0   0   0  59   1   0   0   0  84   0   0   0   0   1   0   0   0]\n",
            " [  0   0   0  64   0   0   0   0   0   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0  79   0   0   0   0   0   0  42   0   0   0   0   0   0]\n",
            " [  0   1   0  46   1   0   0   0   0   0   0  94   0   2   0   0   0]\n",
            " [  0   0   0  94   0   0   0   0   0   0   0   0  42   0   0   0   0]\n",
            " [  0   0   0  38   0   0   0   0   0   0   0   0   0 122   0   0   0]\n",
            " [  0   0   0  60   0   0   0   0   0   0   0   0   0   0  73   0   0]\n",
            " [  0   0   0  58   0   0   0   0   0   0   0   0   0   0   0  29   0]\n",
            " [  0   0   0  97   0   0   0   0   0   0   0   0   0   0   0   0   8]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Arabic       1.00      0.32      0.49       106\n",
            "      Danish       0.94      0.21      0.34        73\n",
            "       Dutch       1.00      0.28      0.44       111\n",
            "     English       0.23      1.00      0.37       291\n",
            "      French       0.99      0.90      0.94       219\n",
            "      German       1.00      0.12      0.21        93\n",
            "       Greek       1.00      0.10      0.19        68\n",
            "       Hindi       0.00      0.00      0.00        10\n",
            "     Italian       1.00      0.58      0.73       145\n",
            "     Kannada       0.67      0.03      0.06        66\n",
            "   Malayalam       1.00      0.35      0.52       121\n",
            "  Portugeese       1.00      0.65      0.79       144\n",
            "     Russian       0.98      0.31      0.47       136\n",
            "     Spanish       0.98      0.76      0.86       160\n",
            "    Sweedish       0.97      0.55      0.70       133\n",
            "       Tamil       0.97      0.33      0.50        87\n",
            "     Turkish       1.00      0.08      0.14       105\n",
            "\n",
            "    accuracy                           0.52      2068\n",
            "   macro avg       0.87      0.39      0.46      2068\n",
            "weighted avg       0.87      0.52      0.54      2068\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = XGBRFClassifier()\n",
        "model2.fit(x_train, y_train)\n",
        "prediction2=model2.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JOtsx5PrYfr",
        "outputId": "e0840796-4dc6-4671-c33f-25279b833025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.confusion_matrix(y_test,prediction2))\n",
        "print(metrics.classification_report(y_test,prediction2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkvcg48csnY9",
        "outputId": "d2a85408-5162-4f85-804b-a1c3b893b3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 80   0   0   0   0   0   0   0   0   0   0   0   0  26   0   0   0]\n",
            " [  0  47   0   0   6   0   0   0   0   0   0   0   0  20   0   0   0]\n",
            " [  0   4  82   1   0   0   0   0   0   0   0   0   0  24   0   0   0]\n",
            " [  0   0   0 262   0   0   0   0   1   0   0   0   0  28   0   0   0]\n",
            " [  0   0   1   0 178   0   0   0   2   0   0   0   0  38   0   0   0]\n",
            " [  0   0   0   0   1  66   0   0   0   0   0   1   0  25   0   0   0]\n",
            " [  0   0   0   1   0   0  51   0   0   0   0   0   0  16   0   0   0]\n",
            " [  0   0   0   0   0   0   0   7   0   0   0   0   0   3   0   0   0]\n",
            " [  0   0   0   0   3   0   0   0 106   0   0   0   0  36   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  39   0   0   0  27   0   0   0]\n",
            " [  0   0   0   1   0   0   0   0   0   0  90   0   0  30   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 102   0  42   0   0   0]\n",
            " [  0   0   0   1   0   0   0   0   0   0   0   0  79  56   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 160   0   0   0]\n",
            " [  0   0   0   2   0   0   0   0   0   0   0   0   0  28 103   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  10   0  77   0]\n",
            " [  0   0   0   0   3   0   0   0   0   0   0   0   0  36   0   0  66]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Arabic       1.00      0.75      0.86       106\n",
            "      Danish       0.92      0.64      0.76        73\n",
            "       Dutch       0.99      0.74      0.85       111\n",
            "     English       0.98      0.90      0.94       291\n",
            "      French       0.93      0.81      0.87       219\n",
            "      German       1.00      0.71      0.83        93\n",
            "       Greek       1.00      0.75      0.86        68\n",
            "       Hindi       1.00      0.70      0.82        10\n",
            "     Italian       0.97      0.73      0.83       145\n",
            "     Kannada       1.00      0.59      0.74        66\n",
            "   Malayalam       1.00      0.74      0.85       121\n",
            "  Portugeese       0.99      0.71      0.83       144\n",
            "     Russian       1.00      0.58      0.73       136\n",
            "     Spanish       0.26      1.00      0.42       160\n",
            "    Sweedish       1.00      0.77      0.87       133\n",
            "       Tamil       1.00      0.89      0.94        87\n",
            "     Turkish       1.00      0.63      0.77       105\n",
            "\n",
            "    accuracy                           0.77      2068\n",
            "   macro avg       0.94      0.74      0.81      2068\n",
            "weighted avg       0.93      0.77      0.81      2068\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##MULTINOMIAL NB CON TF IDF\n",
        "model_tfidf=MultinomialNB()\n",
        "model_tfidf.fit(x1_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzqxGcK-usZD",
        "outputId": "391ed315-8b9d-4344-8776-2d6c9b66a7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_tfidf=model_tfidf.predict(x1_test)"
      ],
      "metadata": {
        "id": "A_ZtI-D8u_R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.confusion_matrix(y_test, prediction_tfidf))\n",
        "print(metrics.classification_report(y_test,prediction_tfidf))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sycH-At3vJfh",
        "outputId": "45c4ef54-20c1-46c7-f4c9-8cf232451b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 97   0   0   9   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0  63   0   3   2   0   0   0   0   0   0   0   0   0   5   0   0]\n",
            " [  0   0 104   3   4   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0 291   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   1 217   0   0   0   0   0   0   0   0   1   0   0   0]\n",
            " [  0   0   0   5   1  87   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   8   0   0  60   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   6   0   0   0   4   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   4   1   0   0   0 139   0   0   0   0   1   0   0   0]\n",
            " [  0   0   0   2   0   0   0   0   0  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   3   0   0   0   0   0   0 118   0   0   0   0   0   0]\n",
            " [  0   0   0   4   0   0   0   0   0   0   0 137   0   3   0   0   0]\n",
            " [  0   0   0  10   0   0   0   0   0   0   0   0 126   0   0   0   0]\n",
            " [  0   0   0   4   1   0   0   0   0   0   0   0   0 155   0   0   0]\n",
            " [  0   1   0   2   0   0   0   0   0   0   0   0   0   0 130   0   0]\n",
            " [  0   0   0   1   0   0   0   0   0   0   0   0   0   0   0  86   0]\n",
            " [  0   0   0  18   4   0   0   0   0   0   0   0   0   0   1   0  82]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Arabic       1.00      0.92      0.96       106\n",
            "      Danish       0.98      0.86      0.92        73\n",
            "       Dutch       1.00      0.94      0.97       111\n",
            "     English       0.78      1.00      0.88       291\n",
            "      French       0.94      0.99      0.97       219\n",
            "      German       1.00      0.94      0.97        93\n",
            "       Greek       1.00      0.88      0.94        68\n",
            "       Hindi       1.00      0.40      0.57        10\n",
            "     Italian       1.00      0.96      0.98       145\n",
            "     Kannada       1.00      0.97      0.98        66\n",
            "   Malayalam       1.00      0.98      0.99       121\n",
            "  Portugeese       1.00      0.95      0.98       144\n",
            "     Russian       1.00      0.93      0.96       136\n",
            "     Spanish       0.97      0.97      0.97       160\n",
            "    Sweedish       0.96      0.98      0.97       133\n",
            "       Tamil       1.00      0.99      0.99        87\n",
            "     Turkish       1.00      0.78      0.88       105\n",
            "\n",
            "    accuracy                           0.95      2068\n",
            "   macro avg       0.98      0.91      0.93      2068\n",
            "weighted avg       0.96      0.95      0.95      2068\n",
            "\n"
          ]
        }
      ]
    }
  ]
}